This directory is for an ad-hoc problem to build and train tic-tac-toe players.

class TicTacToe() is the game board for tic-tac-toe

class EpsGreedyTTTAgent() is a player that uses Monte Carlo to update action-values and learns an epsilon-greedy policy with fixed epsilon.
class EpsGreedyDecayTTTAgent() is a player that uses Monte Carlo to update the action-values and learn an epsilon-greedy policy that starts with a larger epislon and decays to a fixed, smaller epsilon.
class EpsGreedyDecayQAgent() is the same sa EpsGreedyDecayTTTAgent, but it uses Q-learning.

class TTTenv() creates a simple environment for agents to play each other or a human to play an agent.
class TTTournament() creates a tournament of agents that play each other.

The simplest way to use these agents is:
  t = TTTenv()
  t.pl_n_games(1000000)  (note: this takes about 2 minutes on my MacBookPro)
  t.play_human()
